{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLTK Data Cleaning / Exploration\n",
    "\n",
    "Before diving into the Epistles, I've spent the week getting more familiar with some of the tools to process Classical texts in Python, my language of choice. Specifically, I've experimented with loading the texts, cleaning the data, and generating different representations of each document - centered around the problem of classifying sentences as either Xenophon or Plutarch. Next week, I'll work on developing the classification models themselves on this problem, given that we can more easily benchmark the success of classification models between Xenophon and Plutarch because those authors' works are not contested. Once I explore the models there, I see what works and identify good candidates for models to solve the more difficult problem of classifying Plato's Epistles (authorship unknown, genre different than other works by Plato).\n",
    "\n",
    "Ultimately, many of the decisions in this notebook (lemmatization, text normalization) are temporary and intended to get simple models up and running quickly, and I'll be examining them more closely over the next few months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the Corpus\n",
    "\n",
    "Acquiring the documents proved to be a simple task with the CLTK's Corpus Importer (which also allows users to import pre-trained word vectors and Greek-specific data cleaning functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "\n",
    "corpus_importer = CorpusImporter('greek')\n",
    "\n",
    "corpus_importer.import_corpus(\"greek_text_perseus\")\n",
    "corpus_importer.import_corpus(\"greek_text_first1kgreek\")\n",
    "corpus_importer.import_corpus(\"greek_models_cltk\")\n",
    "corpus_importer.import_corpus(\"greek_word2vec_cltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Paragraph': [],\n",
    "        'Author':[]}\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['Paragraph','Author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "At this step, I converted the JSON-style hierarchical documents into lists of strings which denote separate paragraphs. I also took advantage of CLTK's data cleaning formats which remove superfluous punctuation (tailored to Perseus text), and normalize different representations of accented characters (polytonic vs monotonic Greek characters). Since capitalization in Greek is more or less restricted to proper nouns, I dedided not to case-normalize the text explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.formatter import tlg_plaintext_cleanup, cltk_normalize\n",
    "\n",
    "def process_document(doc):\n",
    "    cleaned_paragraphs = []\n",
    "    for paragraph in doc['text'].values():\n",
    "        if type(paragraph) != str:\n",
    "            for sent in paragraph.values():\n",
    "                if type(sent) == str:\n",
    "                    cleaned_sentences.append(cltk_normalize(tlg_plaintext_cleanup(sent)))\n",
    "        else:\n",
    "            cleaned_sentences.append(cltk_normalize(tlg_plaintext_cleanup(paragraph)))\n",
    "    return cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "perseus_reader = get_corpus_reader(corpus_name='greek_text_perseus', language='greek')\n",
    "\n",
    "plutarch_docs = []\n",
    "xenophon_docs = []\n",
    "    \n",
    "for doc in perseus_reader.docs():\n",
    "    if doc[\"author\"] == 'homer':\n",
    "        for paragraph in process_document(doc):\n",
    "            df = df.append({\"Paragraph\": paragraph, \"Author\": \"Homer\"}, ignore_index=True)\n",
    "    if doc[\"author\"] == \"hesiod\":\n",
    "        for paragraph in process_document(doc):\n",
    "            df = df.append({\"Paragraph\": paragraph, \"Author\": \"Hesiod\"}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26458"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[-14][\"Paragraph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author\n",
       "Hesiod       10.875620\n",
       "Homer     21518.083333\n",
       "Name: Paragraph_length, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Paragraph_length\"] = df[\"Paragraph\"].str.len()\n",
    "df.groupby(by=\"Author\")[\"Paragraph_length\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "text_train, text_test, author_train, author_test = train_test_split(df[\"Paragraph\"], df[\"Author\"], test_size=0.3)\n",
    "\n",
    "X = pd.concat([text_train, author_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "plutarch = X[X.Author == \"Homer\"]\n",
    "xenophon = X[X.Author == \"Hesiod\"]\n",
    "\n",
    "# upsample minority\n",
    "xenophon_upsampled = resample(xenophon, replace=True, n_samples=len(plutarch))\n",
    "upsampled = pd.concat([plutarch, xenophon_upsampled])\n",
    "\n",
    "text_train = upsampled[\"Paragraph\"]\n",
    "author_train = upsampled[\"Author\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Word Representation\n",
    "\n",
    "The next step is to transform document into vectorized representations.\n",
    "\n",
    "One popular representation is the bag of words model, in which each document is represented as a vector of length *m*, where *m* is the number of unique words in the vocabulary. The value of each index of the vector is equal to the frequency of the \n",
    "\n",
    "The next representation is the TFIDF model, in which each document is also represented s a vector of length *m*; however, the value at each index of the vector is now assigned a score corresponding to how important that word is to the document - a score directly proportional to the word's frequency in the document and inversely proportional to the word's frequency in the entire document corpus at large.\n",
    "\n",
    "Finally, I've examined the possibility of using gensim to load pre-trained Greek word embeddings - trained by the CLTK team, to my knowledge, through n-grams. Alternatively, I intend to train my own word embeddings through more advanced neural methods. \n",
    "\n",
    "In this process, I made the decision to use a lemmatizer, which reduces each form to its morphological root. Given that Greek nouns, adjectives, and especially verbs can take up to hundreds of different morphological forms, I thought this would be an appropriate choice. However, this process comes at the expense of losing valuable semantic information - that is to say, the sentences \"X sees Y\" and \"Y sees X\" would be rendered the same. One of my research goals is to ponder this tradeoff more intently to formulate a method which preserves both word semantics semantics and morphology as much as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "lemmatizer = LemmaReplacer('greek')\n",
    "\n",
    "analyze_text = lambda x: lemmatizer.lemmatize(x)\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,1), tokenizer=analyze_text)\n",
    "bag_of_words = cv.fit_transform(text_train)\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range = (1,1), tokenizer=analyze_text)\n",
    "tfidf_train = tf.fit_transform(text_train)\n",
    "tfidf_test = tf.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model = Word2Vec.load(\"/Users/blissperry/cltk_data/greek/model/greek_word2vec_cltk/greek_s100_w30_min5_sg.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Classification Models (Part 2 - Coming Soon)\n",
    "\n",
    "Roughly in order of complexity: \n",
    "- (Unigram) Naive Bayes\n",
    "- Straight N-gram model\n",
    "- RNN language model (then, with LSTM) \n",
    "- Attention-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Hesiod       1.00      1.00      1.00       364\n",
      "       Homer       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00       371\n",
      "   macro avg       1.00      1.00      1.00       371\n",
      "weighted avg       1.00      1.00      1.00       371\n",
      "\n",
      "[[364   0]\n",
      " [  0   7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "naive_bayes = MultinomialNB().fit(tfidf_train, author_train)\n",
    "author_pred = naive_bayes.predict(tfidf_test)\n",
    "\n",
    "print(metrics.classification_report(author_test, author_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(author_test, author_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13992\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               1399300   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 1,400,332\n",
      "Trainable params: 1,400,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "# vectorize_layer = TextVectorization(\n",
    "#     standardize=analyze_test,\n",
    "#     max_tokens=max_features,\n",
    "#     output_mode=\"tf-idf\",\n",
    "# )\n",
    "# vectorize_layer.adapt(text_train[\"Paragraph\"])\n",
    "\n",
    "VOCAB_SIZE = tfidf_train.shape[1]\n",
    "print(VOCAB_SIZE)\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(VOCAB_SIZE,)))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(\"adam\", CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 2)\n",
      "(34, 13992)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_author_train = encoder.fit_transform(author_train)\n",
    "encoded_author_test = encoder.transform(author_test)\n",
    "\n",
    "encoded_author_train = to_categorical(encoded_author_train)\n",
    "encoded_author_test = to_categorical(encoded_author_test)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "print(encoded_author_train.shape)\n",
    "print(tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Empty training data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bc1043c44063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     validation_data=(tfidf_test, encoded_author_test))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Empty training data.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Empty training data."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(X, y, batch_size):\n",
    "    number_of_batches = len(text_train)/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X =  X[shuffle_index, :]\n",
    "    y =  y[shuffle_index]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].todense()\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0\n",
    "\n",
    "model.fit_generator(generator=batch_generator(tfidf_train, np.array(encoded_author_train), 128),\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=len(text_train)//128,\n",
    "                    validation_data=(tfidf_test, encoded_author_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_test_pred = model.predict(tfidf_test)\n",
    "author_test_pred = (author_test_pred > 0.5).argmax(axis=1)\n",
    "\n",
    "print(metrics.classification_report(author_test_pred, author_test==\"Xenophon\"))\n",
    "print(metrics.confusion_matrix(author_test_pred, author_test==\"Xenophon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_tf",
   "language": "python",
   "name": "thesis_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
