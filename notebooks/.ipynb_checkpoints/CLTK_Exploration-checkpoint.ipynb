{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLTK Data Cleaning / Exploration\n",
    "\n",
    "Before diving into the Epistles, I've spent the week getting more familiar with some of the tools to process Classical texts in Python, my language of choice. Specifically, I've experimented with loading the texts, cleaning the data, and generating different representations of each document - centered around the problem of classifying sentences as either Xenophon or Plutarch. Next week, I'll work on developing the classification models themselves on this problem, given that we can more easily benchmark the success of classification models between Xenophon and Plutarch because those authors' works are not contested. Once I explore the models there, I see what works and identify good candidates for models to solve the more difficult problem of classifying Plato's Epistles (authorship unknown, genre different than other works by Plato).\n",
    "\n",
    "Ultimately, many of the decisions in this notebook (lemmatization, text normalization) are temporary and intended to get simple models up and running quickly, and I'll be examining them more closely over the next few months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the Corpus\n",
    "\n",
    "Acquiring the documents proved to be a simple task with the CLTK's Corpus Importer (which also allows users to import pre-trained word vectors and Greek-specific data cleaning functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "\n",
    "corpus_importer = CorpusImporter('greek')\n",
    "\n",
    "corpus_importer.import_corpus(\"greek_text_perseus\")\n",
    "corpus_importer.import_corpus(\"greek_text_first1kgreek\")\n",
    "corpus_importer.import_corpus(\"greek_models_cltk\")\n",
    "corpus_importer.import_corpus(\"greek_word2vec_cltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Paragraph': [],\n",
    "        'Author':[]}\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['Paragraph','Author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "At this step, I converted the JSON-style hierarchical documents into lists of strings which denote separate paragraphs. I also took advantage of CLTK's data cleaning formats which remove superfluous punctuation (tailored to Perseus text), and normalize different representations of accented characters (polytonic vs monotonic Greek characters). Since capitalization in Greek is more or less restricted to proper nouns, I dedided not to case-normalize the text explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.formatter import tlg_plaintext_cleanup, cltk_normalize\n",
    "\n",
    "def process_document(doc):\n",
    "    cleaned_paragraphs = []\n",
    "    for paragraph in doc['text'].values():\n",
    "        para_string = \"\"\n",
    "        if type(paragraph) != str:\n",
    "            for sent in paragraph.values():\n",
    "                if type(sent) == str:\n",
    "                    para_string += tlg_plaintext_cleanup(sent)\n",
    "        sentence = cltk_normalize(para_string)\n",
    "        cleaned_paragraphs.append(sentence)\n",
    "        # cleaned_paragraphs.append(lemmatizer.lemmatize(sentence))\n",
    "    return cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perseus_reader = get_corpus_reader(corpus_name='greek_text_perseus', language='greek')\n",
    "\n",
    "plutarch_docs = []\n",
    "xenophon_docs = []\n",
    "    \n",
    "for doc in perseus_reader.docs():\n",
    "    if doc[\"author\"] == 'plutarch':\n",
    "        for paragraph in process_document(doc):\n",
    "            df = df.append({\"Paragraph\": paragraph, \"Author\": \"Plutarch\"}, ignore_index=True)\n",
    "    if doc[\"author\"] == \"xenophon\":\n",
    "        for paragraph in process_document(doc):\n",
    "            df = df.append({\"Paragraph\": paragraph, \"Author\": \"Xenophon\"}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2490"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[-14][\"Paragraph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ἐμοὶ τῆς τῶν βίων ἅψασθαι μὲν γραφῆς συνέβη δ...</td>\n",
       "      <td>Plutarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>τὸν Αἰμιλίων οἶκον ἐν Ῥώμῃ τῶν εὐπατριδῶν γεγ...</td>\n",
       "      <td>Plutarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>πρώτην γοῦν τῶν ἐπιφανῶν ἀρχῶν ἀγορανομίαν με...</td>\n",
       "      <td>Plutarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>συστάντος δὲ τοῦ πρὸς Ἀντίοχον τὸν μέγαν πολέ...</td>\n",
       "      <td>Plutarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ἔγημε δὲ Παπιρίαν, ἀνδρὸς ὑπατικοῦ Μάσωνος θυ...</td>\n",
       "      <td>Plutarch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Paragraph    Author\n",
       "0   ἐμοὶ τῆς τῶν βίων ἅψασθαι μὲν γραφῆς συνέβη δ...  Plutarch\n",
       "1   τὸν Αἰμιλίων οἶκον ἐν Ῥώμῃ τῶν εὐπατριδῶν γεγ...  Plutarch\n",
       "2   πρώτην γοῦν τῶν ἐπιφανῶν ἀρχῶν ἀγορανομίαν με...  Plutarch\n",
       "3   συστάντος δὲ τοῦ πρὸς Ἀντίοχον τὸν μέγαν πολέ...  Plutarch\n",
       "4   ἔγημε δὲ Παπιρίαν, ἀνδρὸς ὑπατικοῦ Μάσωνος θυ...  Plutarch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "text_train, text_test, author_train, author_test = train_test_split(df[\"Paragraph\"], df[\"Author\"], test_size=0.3)\n",
    "\n",
    "X = pd.concat([text_train, author_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "plutarch = X[X.Author == \"Plutarch\"]\n",
    "xenophon = X[X.Author == \"Xenophon\"]\n",
    "\n",
    "# upsample minority\n",
    "xenophon_upsampled = resample(xenophon, replace=True, n_samples=len(plutarch))\n",
    "upsampled = pd.concat([plutarch, xenophon_upsampled])\n",
    "\n",
    "text_train = upsampled[\"Paragraph\"]\n",
    "author_train = upsampled[\"Author\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Word Representation\n",
    "\n",
    "The next step is to transform document into vectorized representations.\n",
    "\n",
    "One popular representation is the bag of words model, in which each document is represented as a vector of length *m*, where *m* is the number of unique words in the vocabulary. The value of each index of the vector is equal to the frequency of the \n",
    "\n",
    "The next representation is the TFIDF model, in which each document is also represented s a vector of length *m*; however, the value at each index of the vector is now assigned a score corresponding to how important that word is to the document - a score directly proportional to the word's frequency in the document and inversely proportional to the word's frequency in the entire document corpus at large.\n",
    "\n",
    "Finally, I've examined the possibility of using gensim to load pre-trained Greek word embeddings - trained by the CLTK team, to my knowledge, through n-grams. Alternatively, I intend to train my own word embeddings through more advanced neural methods. \n",
    "\n",
    "In this process, I made the decision to use a lemmatizer, which reduces each form to its morphological root. Given that Greek nouns, adjectives, and especially verbs can take up to hundreds of different morphological forms, I thought this would be an appropriate choice. However, this process comes at the expense of losing valuable semantic information - that is to say, the sentences \"X sees Y\" and \"Y sees X\" would be rendered the same. One of my research goals is to ponder this tradeoff more intently to formulate a method which preserves both word semantics semantics and morphology as much as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "lemmatizer = LemmaReplacer('greek')\n",
    "\n",
    "analyze_text = lambda x: lemmatizer.lemmatize(x)\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,1), tokenizer=analyze_text)\n",
    "bag_of_words = cv.fit_transform(text_train)\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range = (1,1), tokenizer=analyze_text)\n",
    "tfidf_train = tf.fit_transform(text_train)\n",
    "tfidf_test = tf.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model = Word2Vec.load(\"/Users/blissperry/cltk_data/greek/model/greek_word2vec_cltk/greek_s100_w30_min5_sg.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Classification Models (Part 2 - Coming Soon)\n",
    "\n",
    "Roughly in order of complexity: \n",
    "- (Unigram) Naive Bayes\n",
    "- Straight N-gram model\n",
    "- RNN language model (then, with LSTM) \n",
    "- Attention-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Plutarch       0.90      1.00      0.95       162\n",
      "    Xenophon       1.00      0.62      0.77        48\n",
      "\n",
      "    accuracy                           0.91       210\n",
      "   macro avg       0.95      0.81      0.86       210\n",
      "weighted avg       0.92      0.91      0.91       210\n",
      "\n",
      "[[162   0]\n",
      " [ 18  30]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "naive_bayes = MultinomialNB().fit(tfidf_train, author_train)\n",
    "author_pred = naive_bayes.predict(tfidf_test)\n",
    "\n",
    "print(metrics.classification_report(author_test, author_pred))\n",
    "\n",
    "print(metrics.confusion_matrix(author_test, author_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15188\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 100)               1518900   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 1,519,932\n",
      "Trainable params: 1,519,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "# vectorize_layer = TextVectorization(\n",
    "#     standardize=analyze_test,\n",
    "#     max_tokens=max_features,\n",
    "#     output_mode=\"tf-idf\",\n",
    "# )\n",
    "# vectorize_layer.adapt(text_train[\"Paragraph\"])\n",
    "\n",
    "VOCAB_SIZE = tfidf_train.shape[1]\n",
    "print(VOCAB_SIZE)\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(VOCAB_SIZE,)))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(\"adam\", CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 2)\n",
      "(736, 15188)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_author_train = encoder.fit_transform(author_train)\n",
    "encoded_author_test = encoder.transform(author_test)\n",
    "\n",
    "encoded_author_train = to_categorical(encoded_author_train)\n",
    "encoded_author_test = to_categorical(encoded_author_test)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "print(encoded_author_train.shape)\n",
    "print(tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.6830 - accuracy: 0.6656 - val_loss: 0.6609 - val_accuracy: 0.8571\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 130ms/step - loss: 0.6493 - accuracy: 0.8125 - val_loss: 0.6188 - val_accuracy: 0.8952\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.6000 - accuracy: 0.8422 - val_loss: 0.5711 - val_accuracy: 0.8952\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 128ms/step - loss: 0.5520 - accuracy: 0.8203 - val_loss: 0.5242 - val_accuracy: 0.8952\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5076 - accuracy: 0.8172 - val_loss: 0.4827 - val_accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc1679a10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(X, y, batch_size):\n",
    "    number_of_batches = len(text_train)/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X =  X[shuffle_index, :]\n",
    "    y =  y[shuffle_index]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].todense()\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0\n",
    "\n",
    "model.fit_generator(generator=batch_generator(tfidf_train, np.array(encoded_author_train), 128),\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=len(text_train)//128,\n",
    "                    validation_data=(tfidf_test, encoded_author_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       168\n",
      "           1       0.88      1.00      0.93        42\n",
      "\n",
      "    accuracy                           0.97       210\n",
      "   macro avg       0.94      0.98      0.96       210\n",
      "weighted avg       0.97      0.97      0.97       210\n",
      "\n",
      "[[162   6]\n",
      " [  0  42]]\n"
     ]
    }
   ],
   "source": [
    "author_test_pred = model.predict(tfidf_test)\n",
    "author_test_pred = (author_test_pred > 0.5).argmax(axis=1)\n",
    "\n",
    "print(metrics.classification_report(author_test_pred, author_test==\"Xenophon\"))\n",
    "print(metrics.confusion_matrix(author_test_pred, author_test==\"Xenophon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_tf",
   "language": "python",
   "name": "thesis_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
