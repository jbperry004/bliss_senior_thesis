{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLTK Data Cleaning / Exploration\n",
    "\n",
    "Before diving into the Epistles, I've spent the week getting more familiar with some of the tools to process Classical texts in Python, my language of choice. Specifically, I've experimented with loading the texts, cleaning the data, and generating different representations of each document - centered around the problem of classifying sentences as either Xenophon or Plutarch. Next week, I'll work on developing the classification models themselves on this problem, given that we can more easily benchmark the success of classification models between Xenophon and Plutarch because those authors' works are not contested. Once I explore the models there, I see what works and identify good candidates for models to solve the more difficult problem of classifying Plato's Epistles (authorship unknown, genre different than other works by Plato)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the Corpus\n",
    "\n",
    "Acquiring the documents proved to be a simple task with the CLTK's Corpus Importer (which also allows users to import pre-trained word vectors and Greek-specific data cleaning functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "\n",
    "corpus_importer = CorpusImporter('greek')\n",
    "\n",
    "corpus_importer.import_corpus(\"greek_text_perseus\")\n",
    "corpus_importer.import_corpus(\"greek_text_first1kgreek\")\n",
    "corpus_importer.import_corpus(\"greek_models_cltk\")\n",
    "corpus_importer.import_corpus(\"greek_word2vec_cltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Paragraph': [],\n",
    "        'Author':[]}\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['Paragraph','Author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "At this step, I converted the JSON-style hierarchical documents into lists of strings which denote separate paragraphs. I also took advantage of CLTK's data cleaning formats which remove superfluous punctuation (tailored to Perseus text), and normalize different representations of accented characters (polytonic vs monotonic Greek characters). Since capitalization in Greek is more or less restricted to proper nouns, I dedided not to case-normalize the text explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.formatter import tlg_plaintext_cleanup, cltk_normalize\n",
    "\n",
    "def process_document(doc):\n",
    "    cleaned_paragraphs = []\n",
    "    for paragraph in doc['text'].values():\n",
    "        para_string = \"\"\n",
    "        if type(paragraph) != str:\n",
    "            for sent in paragraph.values():\n",
    "                if type(sent) == str:\n",
    "                    para_string += tlg_plaintext_cleanup(sent)\n",
    "        sentence = cltk_normalize(para_string)\n",
    "        cleaned_paragraphs.append(sentence)\n",
    "        # cleaned_paragraphs.append(lemmatizer.lemmatize(sentence))\n",
    "    return cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perseus_reader = get_corpus_reader(corpus_name='greek_text_perseus', language='greek')\n",
    "\n",
    "plutarch_docs = []\n",
    "xenophon_docs = []\n",
    "    \n",
    "for doc in perseus_reader.docs():\n",
    "    if doc[\"author\"] == 'plutarch':\n",
    "        for paragraph in process_document(doc):\n",
    "            df = df.append({\"Paragraph\": paragraph, \"Author\": \"Plutarch\"}, ignore_index=True)\n",
    "    if doc[\"author\"] == \"xenophon\":\n",
    "        for paragraph in process_document(doc):\n",
    "            df = df.append({\"Paragraph\": paragraph, \"Author\": \"Xenophon\"}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "len(df[df[\"Author\"] == \"Xenophon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Word Representation\n",
    "\n",
    "The next step is to transform document into vectorized representations.\n",
    "\n",
    "One popular representation is the bag of words model, in which each document is represented as a vector of length *m*, where *m* is the number of unique words in the vocabulary. The value of each index of the vector is equal to the frequency of the \n",
    "\n",
    "The next representation is the TFIDF model, in which each document is also represented s a vector of length *m*; however, the value at each index of the vector is now assigned a score corresponding to how important that word is to the document - a score directly proportional to the word's frequency in the document and inversely proportional to the word's frequency in the entire document corpus at large.\n",
    "\n",
    "Finally, I've examined the possibility of using gensim to load pre-trained Greek word embeddings - trained by the CLTK team, to my knowledge, through n-grams. Alternatively, I intend to train my own word embeddings through more advanced neural methods. \n",
    "\n",
    "In this process, I made the decision to use a lemmatizer, which reduces each form to its morphological root. Given that Greek nouns, adjectives, and especially verbs can take up to hundreds of different morphological forms, I thought this would be an appropriate choice. However, this process comes at the expense of losing valuable semantic information - that is to say, the sentences \"X sees Y\" and \"Y sees X\" would be rendered the same. One of my research goals is to ponder this tradeoff more intently to formulate a method which preserves both word semantics semantics and morphology as much as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "lemmatizer = LemmaReplacer('greek')\n",
    "\n",
    "\n",
    "analyze_text = lambda x: lemmatizer.lemmatize(x)\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,1), tokenizer=analyze_text)\n",
    "bag_of_words = cv.fit_transform(df[\"Paragraph\"])\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range = (1,1), tokenizer=analyze_text)\n",
    "tfidf = tf.fit_transform(df['Paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model = Word2Vec.load(\"/Users/blissperry/cltk_data/greek/model/greek_word2vec_cltk/greek_s100_w30_min5_sg.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Classification Models (Part 2 - Coming Soon)\n",
    "\n",
    "Roughly in order of complexity: \n",
    "- (Unigram) Naive Bayes\n",
    "- Straight N-gram model\n",
    "- RNN language model (with LSTM) \n",
    "- Attention-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
